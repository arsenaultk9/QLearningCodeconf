import gymnasium as gym
import numpy as np

# Cr√©ation de l'environnement FrozenLake (non glissant pour simplifier la d√©monstration)
game = "FrozenLake-v1"
env = gym.make(game, is_slippery=False, render_mode="human")
num_episodes = 1000  # Nombre d'√©pisodes r√©duit pour la d√©mo
show_every = 100

# R√©cup√©ration du nombre d'√©tats et d'actions
n_states = env.observation_space.n
n_actions = env.action_space.n

# Initialisation de la Q-table
Q_table = np.zeros((n_states, n_actions))

# Param√®tres d'apprentissage
learning_rate = 5  # Taux d'apprentissage plus √©lev√© pour des mises √† jour plus rapides
discount_factor = (
    0.5  # Facteur d'actualisation abaiss√© pour privil√©gier les r√©compenses imm√©diates
)
max_steps = 30  # Nombre maximal d'√©tapes par √©pisode r√©duit

# Param√®tres d'exploration (epsilon-greedy)
epsilon = 0.7  # D√©but avec une forte exploration
min_epsilon = 0.0000009
# On utilisera une d√©croissance multiplicative pour une diminution progressive plus rapide
epsilon_decay = 0.99

def select_action(state):
    # S√©lection de l'action avec strat√©gie epsilon-greedy
    if np.random.uniform(0, 1) < epsilon:
        action = env.action_space.sample()
    else:
        action = np.argmax(Q_table[state, :])
        
    return action

def optimize_q_function(state, action, next_state, reward):
    # Mise √† jour de la Q-table selon la formule du Q-learning
    # Belman equation: ùëÑ(ùë†,ùëé)‚ÜêùëÑ(ùë†,ùëé)+ùõº( ùëü+ ùõæmaxùëé‚Ä≤ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)‚àíùëÑ(ùë†,ùëé))
    
    Q_table[state, action] = Q_table[state, action] + learning_rate * (
        reward
        + discount_factor * np.max(Q_table[new_state, :])
        - Q_table[state, action]
    )

for episode in range(num_episodes):
    show_episode = episode % show_every == 0
    render_mode = "human" if (episode % show_every) == 0 else None

    if show_episode:
        print(f"==== Episode {episode}, epsilon={epsilon}")
        # print("Q Table :")
        # print(Q_table)

    env = gym.make(game, render_mode=render_mode)

    state, _ = env.reset()
    done = False

    for step in range(max_steps):
        action = select_action(state)
        
        new_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        # Reward shaping : si l'√©pisode se termine et que la r√©compense est 0,
        # on consid√®re que l'agent est tomb√© dans un trou et on p√©nalise fortement.
        if done and reward == 0:
            reward = -1

        # Indiqu√© lorsque gagn√©
        if done and reward == 1:
            print(f"Episode {episode} gagn√© en {step} √©tapes")

        optimize_q_function(state, action, new_state, reward)

        state = new_state

        if done:
            break

    # D√©croissance d'epsilon pour r√©duire progressivement l'exploration
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

print("Table Q finale :")
print(Q_table)

# Test de la politique (Strategie) apprise
state, _ = env.reset()
env.render()
done = False
while not done:
    action = np.argmax(Q_table[state, :])
    state, reward, terminated, truncated, info = env.step(action)
    env.render()
    done = terminated or truncated
